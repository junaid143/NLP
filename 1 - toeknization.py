

# tokenization


from nltk.tokenize import sent_tokenize
text = input("Enter a Sentence");
print(sent_tokenize(text))



#Word Tokenization:

from nltk.tokenize import word_tokenize
text = input("Enter a Sentence");
print(word_tokenize(text))


